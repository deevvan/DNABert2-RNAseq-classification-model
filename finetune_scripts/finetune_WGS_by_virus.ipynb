{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1448f771-4eac-4182-9cf5-6d4083127f05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences for each label:\n",
      "Label influenza_b: 75303 sequences\n",
      "Label influenza_a: 71620 sequences\n",
      "Label sars_cov_2: 18636 sequences\n",
      "Label rsv: 6060 sequences\n",
      "Label rhinovirus: 1343 sequences\n"
     ]
    }
   ],
   "source": [
    "# Extracting WGS from fasta files belonging to hCOV19, IAV, IBV, rhinovirus and RSV\n",
    "# Removing sequences with ambigious nucleotides\n",
    "# Mapping virus labels to numerical labels\n",
    "# Write out sequences, respective labels, segment name and EPI ID/Accession ID to output csv\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    if \"hcov-19\" in filename:\n",
    "        return \"sars_cov_2\"\n",
    "    elif \"influenzaA\" in filename:\n",
    "        return \"influenza_a\"\n",
    "    elif \"influenzaB\" in filename:\n",
    "        return \"influenza_b\"\n",
    "    elif \"rsv\" in filename:\n",
    "        return \"rsv\"\n",
    "    elif \"rhino\" in filename:\n",
    "        return \"rhinovirus\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_fasta_file(fasta_file):\n",
    "    valid_nucleotides = {'A', 'T', 'G', 'C'}\n",
    "    label = get_label_from_filename(fasta_file)\n",
    "    if label is None:\n",
    "        return []\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = ''\n",
    "        epi_id = ''\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                if sequence and set(sequence.upper()).issubset(valid_nucleotides):\n",
    "                    sequences.append((sequence.upper(), epi_id, label))\n",
    "                header_parts = line.strip().split('|')\n",
    "                epi_id = header_parts[1] if len(header_parts) > 1 else ''\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        \n",
    "        if sequence and set(sequence.upper()).issubset(valid_nucleotides):\n",
    "            sequences.append((sequence.upper(), epi_id, label))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def label_to_number(label):\n",
    "    label_map = {\n",
    "        \"sars_cov_2\": 1,\n",
    "        \"influenza_a\": 2,\n",
    "        \"influenza_b\": 3,\n",
    "        \"rsv\": 4,\n",
    "        \"rhinovirus\": 5\n",
    "    }\n",
    "    return label_map.get(label, 0)\n",
    "\n",
    "def process_fasta_files(directory, output_csv):\n",
    "    fasta_files = os.popen(f\"ls {directory}/*.fasta\").read().split()\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(process_fasta_file, fasta_files)\n",
    "\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['sequence', 'EPI_ID', 'label_name', 'label_number']\n",
    "        df = pd.DataFrame(columns=fieldnames)\n",
    "        df.to_csv(csv_file, mode='w', index=False, header=True)\n",
    "\n",
    "        for result in results:\n",
    "            for sequence, epi_id, label in result:\n",
    "                label_number = label_to_number(label)\n",
    "                df = pd.DataFrame([[sequence, epi_id, label, label_number]], columns=fieldnames)\n",
    "                df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "    # Load the CSV and count the number of sequences for each label\n",
    "    data = pd.read_csv(output_csv)\n",
    "    label_counts = data['label_name'].value_counts()\n",
    "\n",
    "    print(\"Number of sequences for each label:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} sequences\")\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = \"/path/to/virus/fasta/files/\"\n",
    "\n",
    "process_fasta_files(main_dir, 'WGS_by_virus_finetune.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0186a-fd5f-4cd1-ba11-0c0d2df366be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d57a58-caf0-448b-8fec-bc33070ee054",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [00:27,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## Path1 # create nonoverlapping 250 bps fragments with labels from above output csv\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate XXX bp non-overlapping fragments\n",
    "def generate_non_overlapping_fragments(sequence, fragment_len=250):\n",
    "    fragments = []\n",
    "    for start in range(0, len(sequence), fragment_len):\n",
    "        fragment = sequence[start:start + fragment_len]\n",
    "        if len(fragment) == fragment_len:\n",
    "            fragments.append(fragment)\n",
    "    return fragments\n",
    "\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = \"/path/to/virus/finetune/csv/\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv_path = os.path.join(main_dir, \"WGS_by_virus_finetune.csv\")\n",
    "output_csv_path = os.path.join(main_dir, \"WGS_by_virus_finetune1_250bp_fragments.csv\")\n",
    "\n",
    "# Read the input CSV file in chunks\n",
    "chunksize = 1000  # Adjust the chunk size as needed\n",
    "input_columns = [\"EPI_ID\", \"label_name\", \"label_number\", \"sequence\"]\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"EPI_ID\", \"label_name\", \"label_number\", \"sequence\"])  # Write header\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(input_csv_path, usecols=input_columns, chunksize=chunksize)):\n",
    "        for index, row in chunk.iterrows():\n",
    "            epi_id = row[\"EPI_ID\"]\n",
    "            label_name = row[\"label_name\"]\n",
    "            label_number = row[\"label_number\"]\n",
    "            sequence = row[\"sequence\"]\n",
    "            \n",
    "            fragments = generate_non_overlapping_fragments(sequence)\n",
    "            \n",
    "            for fragment in fragments:\n",
    "                writer.writerow([epi_id, label_name, label_number, fragment])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d713ae-fbae-433c-8c16-ea6a1d814d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = \"WGS_by_virus_finetune.csv\"\n",
    "data = pd.read_csv(input_csv_path,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06c8797-8164-408d-a772-6e4d0bb0bdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>EPI_ID</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTG...</td>\n",
       "      <td>EPI_ISL_8801366</td>\n",
       "      <td>sars_cov_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTG...</td>\n",
       "      <td>EPI_ISL_8801365</td>\n",
       "      <td>sars_cov_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGATC...</td>\n",
       "      <td>EPI_ISL_9404688</td>\n",
       "      <td>sars_cov_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTG...</td>\n",
       "      <td>EPI_ISL_8801370</td>\n",
       "      <td>sars_cov_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGAT...</td>\n",
       "      <td>EPI_ISL_9404748</td>\n",
       "      <td>sars_cov_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence           EPI_ID  \\\n",
       "0  GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTG...  EPI_ISL_8801366   \n",
       "1  GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTG...  EPI_ISL_8801365   \n",
       "2  ACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGATC...  EPI_ISL_9404688   \n",
       "3  GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTG...  EPI_ISL_8801370   \n",
       "4  TACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGAT...  EPI_ISL_9404748   \n",
       "\n",
       "   label_name  label_number  \n",
       "0  sars_cov_2             1  \n",
       "1  sars_cov_2             1  \n",
       "2  sars_cov_2             1  \n",
       "3  sars_cov_2             1  \n",
       "4  sars_cov_2             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06ccd2-8649-4c32-bc16-e83ecf891f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492a4e0b-d117-442e-a115-e5a9beb01e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [01:10,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragments with reverse complements have been written to /mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/intermediate_csvs/WGS_by_virus_5labels_250bp_50overlap_complementary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating 250bps fragments with 50 bp overlaps from WGS generated above\n",
    "# Generating reverse complementary sequence for each generated fragment \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Function to generate overlapping 250 bp fragments\n",
    "def generate_overlapping_fragments(sequence, fragment_len=250, overlap=50):\n",
    "    fragments = []\n",
    "    for start in range(0, len(sequence) - fragment_len + 1, fragment_len - overlap):\n",
    "        fragment = sequence[start:start + fragment_len]\n",
    "        if len(fragment) == fragment_len:\n",
    "            fragments.append(fragment)\n",
    "    return fragments\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "input_dir = \"/path/to/virus/finetune/csv/\"\n",
    "main_dir = \"/path/to/virus/finetune/csv/intermediate/files/\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv_path = os.path.join(\"WGS_by_virus_finetune.csv\")\n",
    "output_csv_path = os.path.join(main_dir, \"WGS_by_virus_5labels_250bp_50overlap_complementary.csv\")\n",
    "\n",
    "# Read the input CSV file in chunks\n",
    "chunksize = 1000  # Adjust the chunk size as needed\n",
    "input_columns = [\"EPI_ID\", \n",
    "                 \"label_name\", \"label_number\", \n",
    "                 \"sequence\"]\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"EPI_ID\", \n",
    "                     \"label_name\", \"label_number\",  \n",
    "                     \"sequence\"])  # Write header\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(input_csv_path, usecols=input_columns, chunksize=chunksize)):\n",
    "        for index, row in chunk.iterrows():\n",
    "            epi_id = row[\"EPI_ID\"]\n",
    "            variant_label = row[\"label_name\"]\n",
    "            variant_label_number = row[\"label_number\"]\n",
    "            sequence = row[\"sequence\"]\n",
    "            \n",
    "            fragments = generate_overlapping_fragments(sequence)\n",
    "            \n",
    "            for fragment in fragments:\n",
    "                # Write the original fragment\n",
    "                writer.writerow([epi_id, variant_label, variant_label_number, fragment])\n",
    "                \n",
    "                # Generate and write the reverse complementary fragment\n",
    "                reverse_complement = str(Seq(fragment).reverse_complement())\n",
    "                writer.writerow([epi_id, variant_label, variant_label_number, reverse_complement])\n",
    "\n",
    "print(f\"Fragments with reverse complements have been written to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0c9f1e-5a24-4827-b0b4-4c4dfa622bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: influenza_a, Unique EPI_ID Count: 9101\n",
      "Label: influenza_b, Unique EPI_ID Count: 9977\n",
      "Label: rhinovirus, Unique EPI_ID Count: 1343\n",
      "Label: rsv, Unique EPI_ID Count: 6058\n",
      "Label: sars_cov_2, Unique EPI_ID Count: 18636\n"
     ]
    }
   ],
   "source": [
    "# Calculating the number of unique EPI IDs for each unique value of virus label_name\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "csv_file_path = '/path/to/virus/finetune/csv/intermediate/files/WGS_by_virus_5labels_250bp_50overlap_complementary.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Group by label_name and count unique EPI_ID values\n",
    "label_counts = data.groupby('label_name')['EPI_ID'].nunique()\n",
    "\n",
    "# Print out the counts\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label: {label}, Unique EPI_ID Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f560bd46-2b24-4aa1-bba1-fae99139661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences and unique EPI ID values for each label:\n",
      "Label influenza_a: 154868 sequences, 1300 unique EPI IDs\n",
      "Label influenza_b: 159818 sequences, 1300 unique EPI IDs\n",
      "Label rhinovirus: 92938 sequences, 1343 unique EPI IDs\n",
      "Label rsv: 178824 sequences, 1300 unique EPI IDs\n",
      "Label sars_cov_2: 384960 sequences, 1300 unique EPI IDs\n"
     ]
    }
   ],
   "source": [
    "# Creating subset of above output csv with desired number of unique EPI_IDs for each virus label_name\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def filter_sequences(input_csv, output_csv):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(input_csv,low_memory=False)\n",
    "    \n",
    "    # Count unique EPI IDs per label_name\n",
    "    unique_epi_counts = data.groupby('label_name')['EPI_ID'].nunique()\n",
    "    \n",
    "    # Find labels with at least 100 unique EPI IDs\n",
    "    valid_labels = unique_epi_counts[unique_epi_counts >= 1300].index\n",
    "    \n",
    "    # Filter the dataset to include only the valid labels\n",
    "    filtered_data = data[data['label_name'].isin(valid_labels)]\n",
    "    \n",
    "    # Initialize a list to store the filtered sequences\n",
    "    filtered_sequences = []\n",
    "    \n",
    "    # For each valid label, select 100 random EPI IDs and filter sequences\n",
    "    for label in valid_labels:\n",
    "        # Get unique EPI IDs for the current label\n",
    "        label_data = filtered_data[filtered_data['label_name'] == label]\n",
    "        unique_ep_ids = label_data['EPI_ID'].unique()\n",
    "        \n",
    "        # Randomly select 1300 unique EPI IDs\n",
    "        if len(unique_ep_ids) > 1300:\n",
    "            selected_ep_ids = pd.Series(unique_ep_ids).sample(n=1300, random_state=1).tolist()\n",
    "        else:\n",
    "            selected_ep_ids = unique_ep_ids\n",
    "        \n",
    "        # Filter data based on selected EPI IDs\n",
    "        selected_data = label_data[label_data['EPI_ID'].isin(selected_ep_ids)]\n",
    "        filtered_sequences.append(selected_data)\n",
    "    \n",
    "    # Concatenate all filtered sequences into a single DataFrame\n",
    "    result_df = pd.concat(filtered_sequences)\n",
    "    \n",
    "    # Save the filtered data to a new CSV file\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print number of sequences and unique EPI ID values for each label_name\n",
    "    filtered_label_counts = result_df.groupby('label_name').agg(\n",
    "        num_sequences=('sequence', 'count'),\n",
    "        num_unique_epi_ids=('EPI_ID', 'nunique')\n",
    "    )\n",
    "    \n",
    "    print(\"Number of sequences and unique EPI ID values for each label:\")\n",
    "    for label, row in filtered_label_counts.iterrows():\n",
    "        print(f\"Label {label}: {row['num_sequences']} sequences, {row['num_unique_epi_ids']} unique EPI IDs\")\n",
    "\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "input_dir =\"/path/to/virus/finetune/csv/intermediate/files/\"\n",
    "main_dir = \"/path/to/virus/finetune/csv/\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv = os.path.join(input_dir, \"WGS_by_virus_5labels_250bp_50overlap_complementary.csv\")\n",
    "output_csv = os.path.join(main_dir, \"WGS_by_virus_5labels_250bp_50overlap_complementary_1300epi.csv\")\n",
    "\n",
    "# Execute the filtering and reporting\n",
    "filter_sequences(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d7cc2-8163-4db4-bd2e-bd81172c7020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
