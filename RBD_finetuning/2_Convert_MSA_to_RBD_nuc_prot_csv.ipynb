{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c78e751",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted sequences between positions 34732 and 36397 to '<_io.TextIOWrapper name='ref_genome_RBD_nucleotides.fasta' mode='w' encoding='UTF-8'>'.\n"
     ]
    }
   ],
   "source": [
    "# Trimming genome msa files to only extract RBD segment and remove gaps to get true RBD nucleotide sequences\n",
    "\n",
    "def extract_sequences_in_range(msa_fasta_file, start_position, end_position, output_file):\n",
    "    with open(msa_fasta_file, \"r\") as file:\n",
    "        header = \"\"\n",
    "        sequence = \"\"\n",
    "\n",
    "        # Open the output file for writing\n",
    "        with open(output_file, \"w\") as output_file:\n",
    "\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "\n",
    "                if line.startswith(\">\"):\n",
    "                    # Start a new sequence\n",
    "                    if header and sequence:\n",
    "                        # Process the previous sequence if it's complete\n",
    "                        sequence = sequence.replace(\"*\", \"\")\n",
    "                        extracted_sequence = sequence[start_position - 1:end_position]\n",
    "\n",
    "                        # Remove gaps from the extracted sequence.\n",
    "                        extracted_sequence = extracted_sequence.replace(\"-\", \"\")\n",
    "\n",
    "                        # Write the extracted sequence to the output file.\n",
    "                        output_file.write(f\">{header}\\n{extracted_sequence}\\n\")\n",
    "\n",
    "                    header = line\n",
    "                    sequence = \"\"\n",
    "                else:\n",
    "                    sequence += line\n",
    "\n",
    "            # Process the last sequence if it's complete\n",
    "            if header and sequence:\n",
    "                sequence = sequence.replace(\"*\", \"\")\n",
    "                extracted_sequence = sequence[start_position - 1:end_position]\n",
    "                extracted_sequence = extracted_sequence.replace(\"-\", \"\")\n",
    "                output_file.write(f\">{header}\\n{extracted_sequence}\\n\")\n",
    "\n",
    "    print(f\"Extracted sequences between positions {start_position} and {end_position} to '{output_file}'.\")\n",
    "\n",
    "def main():\n",
    "    msa_fasta_file = \"your/path/ref_genome_MSA.fasta\"\n",
    "    start_position = 34732\n",
    "    end_position = 36397\n",
    "    output_file = \"your/path/ref_genome_RBD_nucleotides.fasta\"\n",
    "    extract_sequences_in_range(msa_fasta_file, start_position, end_position, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae08ac14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Statistics:\n",
      "Total Sequences: 1660854\n",
      "Total Bases: 1114385340\n",
      "Minimum Length: 0 bases\n",
      "Maximum Length: 1600 bases\n",
      "Average Length: 670.97 bases\n"
     ]
    }
   ],
   "source": [
    "# Length Statistics for trimmed reads\n",
    "from Bio import SeqIO\n",
    "\n",
    "def generate_sequence_statistics(fasta_file):\n",
    "    sequence_lengths = []\n",
    "    total_sequences = 0\n",
    "    total_bases = 0\n",
    "\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            total_sequences += 1\n",
    "            sequence_length = len(record.seq)\n",
    "            sequence_lengths.append(sequence_length)\n",
    "            total_bases += sequence_length\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_length = min(sequence_lengths)\n",
    "    max_length = max(sequence_lengths)\n",
    "    average_length = total_bases / total_sequences\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"Sequence Statistics:\")\n",
    "    print(f\"Total Sequences: {total_sequences}\")\n",
    "    print(f\"Total Bases: {total_bases}\")\n",
    "    print(f\"Minimum Length: {min_length} bases\")\n",
    "    print(f\"Maximum Length: {max_length} bases\")\n",
    "    print(f\"Average Length: {average_length:.2f} bases\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fasta_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/msaCodon_1024_trimmed_RBD_new.fasta\"  # Replace with your FASTA file\n",
    "    generate_sequence_statistics(fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea28618-edfd-4b35-a5c3-f323906f075d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read length: 358, Number of sequences: 1\n",
      "Read length: 380, Number of sequences: 1\n",
      "Read length: 410, Number of sequences: 1\n",
      "Read length: 413, Number of sequences: 4\n",
      "Read length: 417, Number of sequences: 1\n",
      "Read length: 439, Number of sequences: 1\n",
      "Read length: 440, Number of sequences: 1\n",
      "Read length: 441, Number of sequences: 2\n",
      "Read length: 442, Number of sequences: 1\n",
      "Read length: 446, Number of sequences: 1\n",
      "Read length: 452, Number of sequences: 1\n",
      "Read length: 470, Number of sequences: 1\n",
      "Read length: 476, Number of sequences: 1\n",
      "Read length: 482, Number of sequences: 1\n",
      "Read length: 485, Number of sequences: 3\n",
      "Read length: 489, Number of sequences: 1\n",
      "Read length: 493, Number of sequences: 1\n",
      "Read length: 495, Number of sequences: 1\n",
      "Read length: 501, Number of sequences: 1\n",
      "Read length: 502, Number of sequences: 1\n",
      "Read length: 510, Number of sequences: 2\n",
      "Read length: 511, Number of sequences: 1\n",
      "Read length: 519, Number of sequences: 2\n",
      "Read length: 526, Number of sequences: 1\n",
      "Read length: 527, Number of sequences: 2\n",
      "Read length: 530, Number of sequences: 1\n",
      "Read length: 531, Number of sequences: 1\n",
      "Read length: 534, Number of sequences: 1\n",
      "Read length: 535, Number of sequences: 1\n",
      "Read length: 537, Number of sequences: 2\n",
      "Read length: 538, Number of sequences: 1\n",
      "Read length: 540, Number of sequences: 1\n",
      "Read length: 544, Number of sequences: 1\n",
      "Read length: 545, Number of sequences: 3\n",
      "Read length: 546, Number of sequences: 1\n",
      "Read length: 549, Number of sequences: 3\n",
      "Read length: 551, Number of sequences: 1\n",
      "Read length: 552, Number of sequences: 2\n",
      "Read length: 554, Number of sequences: 1\n",
      "Read length: 555, Number of sequences: 4\n",
      "Read length: 556, Number of sequences: 1\n",
      "Read length: 557, Number of sequences: 2\n",
      "Read length: 558, Number of sequences: 1\n",
      "Read length: 559, Number of sequences: 1\n",
      "Read length: 560, Number of sequences: 4\n",
      "Read length: 561, Number of sequences: 1\n",
      "Read length: 562, Number of sequences: 2\n",
      "Read length: 563, Number of sequences: 2\n",
      "Read length: 564, Number of sequences: 5\n",
      "Read length: 565, Number of sequences: 8\n",
      "Read length: 566, Number of sequences: 5\n",
      "Read length: 567, Number of sequences: 6\n",
      "Read length: 569, Number of sequences: 10\n",
      "Read length: 570, Number of sequences: 7\n",
      "Read length: 571, Number of sequences: 6\n",
      "Read length: 572, Number of sequences: 5\n",
      "Read length: 573, Number of sequences: 3\n",
      "Read length: 574, Number of sequences: 7\n",
      "Read length: 575, Number of sequences: 13\n",
      "Read length: 576, Number of sequences: 6\n",
      "Read length: 577, Number of sequences: 10\n",
      "Read length: 578, Number of sequences: 9\n",
      "Read length: 579, Number of sequences: 44\n",
      "Read length: 580, Number of sequences: 29\n",
      "Read length: 581, Number of sequences: 24\n",
      "Read length: 582, Number of sequences: 12\n",
      "Read length: 583, Number of sequences: 9\n",
      "Read length: 584, Number of sequences: 6\n",
      "Read length: 585, Number of sequences: 2\n",
      "Read length: 586, Number of sequences: 3\n",
      "Read length: 587, Number of sequences: 3\n",
      "Read length: 588, Number of sequences: 2\n",
      "Read length: 589, Number of sequences: 3\n",
      "Read length: 590, Number of sequences: 7\n",
      "Read length: 591, Number of sequences: 1\n",
      "Read length: 592, Number of sequences: 1\n",
      "Read length: 593, Number of sequences: 1\n",
      "Read length: 594, Number of sequences: 6\n",
      "Read length: 595, Number of sequences: 2\n",
      "Read length: 597, Number of sequences: 2\n",
      "Read length: 598, Number of sequences: 1\n",
      "Read length: 599, Number of sequences: 2\n",
      "Read length: 600, Number of sequences: 4\n",
      "Read length: 601, Number of sequences: 2\n",
      "Read length: 602, Number of sequences: 6\n",
      "Read length: 603, Number of sequences: 4\n",
      "Read length: 604, Number of sequences: 4\n",
      "Read length: 605, Number of sequences: 4\n",
      "Read length: 606, Number of sequences: 1\n",
      "Read length: 607, Number of sequences: 4\n",
      "Read length: 608, Number of sequences: 5\n",
      "Read length: 611, Number of sequences: 1\n",
      "Read length: 612, Number of sequences: 2\n",
      "Read length: 613, Number of sequences: 1\n",
      "Read length: 614, Number of sequences: 2\n",
      "Read length: 615, Number of sequences: 2\n",
      "Read length: 617, Number of sequences: 4\n",
      "Read length: 618, Number of sequences: 1\n",
      "Read length: 619, Number of sequences: 2\n",
      "Read length: 620, Number of sequences: 3\n",
      "Read length: 621, Number of sequences: 2\n",
      "Read length: 622, Number of sequences: 3\n",
      "Read length: 623, Number of sequences: 3\n",
      "Read length: 624, Number of sequences: 2\n",
      "Read length: 625, Number of sequences: 3\n",
      "Read length: 626, Number of sequences: 3\n",
      "Read length: 627, Number of sequences: 2\n",
      "Read length: 630, Number of sequences: 2\n",
      "Read length: 631, Number of sequences: 3\n",
      "Read length: 632, Number of sequences: 5\n",
      "Read length: 633, Number of sequences: 6\n",
      "Read length: 634, Number of sequences: 2\n",
      "Read length: 635, Number of sequences: 6\n",
      "Read length: 636, Number of sequences: 5\n",
      "Read length: 637, Number of sequences: 5\n",
      "Read length: 638, Number of sequences: 4\n",
      "Read length: 639, Number of sequences: 2\n",
      "Read length: 640, Number of sequences: 4\n",
      "Read length: 641, Number of sequences: 17\n",
      "Read length: 642, Number of sequences: 5\n",
      "Read length: 643, Number of sequences: 16\n",
      "Read length: 644, Number of sequences: 9\n",
      "Read length: 645, Number of sequences: 5\n",
      "Read length: 646, Number of sequences: 6\n",
      "Read length: 647, Number of sequences: 8\n",
      "Read length: 648, Number of sequences: 8\n",
      "Read length: 649, Number of sequences: 4\n",
      "Read length: 650, Number of sequences: 7\n",
      "Read length: 651, Number of sequences: 45\n",
      "Read length: 652, Number of sequences: 7\n",
      "Read length: 653, Number of sequences: 13\n",
      "Read length: 654, Number of sequences: 6\n",
      "Read length: 655, Number of sequences: 3\n",
      "Read length: 656, Number of sequences: 8\n",
      "Read length: 657, Number of sequences: 8\n",
      "Read length: 658, Number of sequences: 5\n",
      "Read length: 659, Number of sequences: 14\n",
      "Read length: 660, Number of sequences: 18\n",
      "Read length: 661, Number of sequences: 31\n",
      "Read length: 662, Number of sequences: 32\n",
      "Read length: 663, Number of sequences: 3\n",
      "Read length: 664, Number of sequences: 6\n",
      "Read length: 665, Number of sequences: 28\n",
      "Read length: 666, Number of sequences: 15\n",
      "Read length: 667, Number of sequences: 18\n",
      "Read length: 668, Number of sequences: 115\n",
      "Read length: 669, Number of sequences: 51\n",
      "Read length: 670, Number of sequences: 356\n",
      "Read length: 671, Number of sequences: 1659465\n",
      "Read length: 672, Number of sequences: 32\n",
      "Read length: 673, Number of sequences: 4\n",
      "Read length: 674, Number of sequences: 7\n",
      "Read length: 675, Number of sequences: 3\n",
      "Read length: 676, Number of sequences: 3\n",
      "Read length: 677, Number of sequences: 2\n",
      "Read length: 682, Number of sequences: 2\n",
      "Read length: 685, Number of sequences: 2\n",
      "Read length: 686, Number of sequences: 1\n",
      "Read length: 687, Number of sequences: 1\n",
      "Read length: 690, Number of sequences: 1\n",
      "Read length: 692, Number of sequences: 3\n",
      "Read length: 693, Number of sequences: 3\n",
      "Read length: 694, Number of sequences: 1\n",
      "Read length: 696, Number of sequences: 1\n",
      "Read length: 697, Number of sequences: 1\n",
      "Read length: 699, Number of sequences: 1\n",
      "Read length: 710, Number of sequences: 2\n",
      "Read length: 1600, Number of sequences: 2\n"
     ]
    }
   ],
   "source": [
    "# Display number of sequences in fasta file with all possible readlengths \n",
    "from collections import defaultdict\n",
    "\n",
    "def count_sequence_lengths(fasta_file):\n",
    "    sequence_lengths = defaultdict(int)\n",
    "    \n",
    "    with open(fasta_file, \"r\") as file:\n",
    "        sequence = \"\"\n",
    "        \n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if sequence:\n",
    "                    sequence_lengths[len(sequence)] += 1\n",
    "                sequence = \"\"\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        \n",
    "        # Don't forget to count the last sequence\n",
    "        if sequence:\n",
    "            sequence_lengths[len(sequence)] += 1\n",
    "    \n",
    "    return sequence_lengths\n",
    "\n",
    "def print_sequence_lengths(fasta_file):\n",
    "    sequence_lengths = count_sequence_lengths(fasta_file)\n",
    "    \n",
    "    for length, count in sorted(sequence_lengths.items()):\n",
    "        print(f\"Read length: {length}, Number of sequences: {count}\")\n",
    "\n",
    "def main():\n",
    "    fasta_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/msaCodon_1024_trimmed_RBD_new.fasta\"\n",
    "    print_sequence_lengths(fasta_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033f9df8-4e7c-44dd-83a2-7cca51dc53e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences between lengths 671 and 671 written to '/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new.fasta'.\n"
     ]
    }
   ],
   "source": [
    "# Calculate first quartile (Q1), third quartile (Q3), and the interquartile range (IQR) and \n",
    "# use the IQR to determine the lower and upper bounds for outlier detection and \n",
    "# remove outlier readlengths.\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_sequence_lengths(fasta_file):\n",
    "    sequence_lengths = []\n",
    "    sequences = []\n",
    "\n",
    "    with open(fasta_file, \"r\") as file:\n",
    "        sequence = \"\"\n",
    "        header = \"\"\n",
    "\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if sequence:\n",
    "                    sequence_lengths.append(len(sequence))\n",
    "                    sequences.append((header, sequence))\n",
    "                header = line.strip()\n",
    "                sequence = \"\"\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "\n",
    "        # Don't forget to count the last sequence\n",
    "        if sequence:\n",
    "            sequence_lengths.append(len(sequence))\n",
    "            sequences.append((header, sequence))\n",
    "\n",
    "    return sequence_lengths, sequences\n",
    "\n",
    "def calculate_iqr_outliers(sequence_lengths):\n",
    "    q1 = np.percentile(sequence_lengths, 25)\n",
    "    q3 = np.percentile(sequence_lengths, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def filter_outliers_and_write(sequences, lower_bound, upper_bound, output_file):\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        for header, sequence in sequences:\n",
    "            if lower_bound <= len(sequence) <= upper_bound:\n",
    "                outfile.write(f\"{header}\\n{sequence}\\n\")\n",
    "\n",
    "def main():\n",
    "    fasta_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/msaCodon_1024_trimmed_RBD_new.fasta\"\n",
    "    output_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new.fasta\"\n",
    "\n",
    "    sequence_lengths, sequences = count_sequence_lengths(fasta_file)\n",
    "    lower_bound, upper_bound = calculate_iqr_outliers(sequence_lengths)\n",
    "    filter_outliers_and_write(sequences, lower_bound, upper_bound, output_file)\n",
    "\n",
    "    print(f\"Sequences between lengths {int(lower_bound)} and {int(upper_bound)} written to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5c1f57-d0f3-42c5-b1b0-ed9d6c1dd888",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Statistics:\n",
      "Total Sequences: 1659465\n",
      "Total Bases: 1113501015\n",
      "Minimum Length: 671 bases\n",
      "Maximum Length: 671 bases\n",
      "Average Length: 671.00 bases\n"
     ]
    }
   ],
   "source": [
    "# Length Statistics for filtered trimmed reads\n",
    "from Bio import SeqIO\n",
    "\n",
    "def generate_sequence_statistics(fasta_file):\n",
    "    sequence_lengths = []\n",
    "    total_sequences = 0\n",
    "    total_bases = 0\n",
    "\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            total_sequences += 1\n",
    "            sequence_length = len(record.seq)\n",
    "            sequence_lengths.append(sequence_length)\n",
    "            total_bases += sequence_length\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_length = min(sequence_lengths)\n",
    "    max_length = max(sequence_lengths)\n",
    "    average_length = total_bases / total_sequences\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"Sequence Statistics:\")\n",
    "    print(f\"Total Sequences: {total_sequences}\")\n",
    "    print(f\"Total Bases: {total_bases}\")\n",
    "    print(f\"Minimum Length: {min_length} bases\")\n",
    "    print(f\"Maximum Length: {max_length} bases\")\n",
    "    print(f\"Average Length: {average_length:.2f} bases\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fasta_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_new.fasta\"  # Replace with your FASTA file\n",
    "    generate_sequence_statistics(fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44caba27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 1600000 valid sequences and saved to /mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.fasta\n"
     ]
    }
   ],
   "source": [
    "## Python script to make a subset of the filtered_msaCodon_1024_trimmed_RBD.fasta without N \n",
    "from Bio import SeqIO\n",
    "import random\n",
    "\n",
    "# Input FASTA file and output file\n",
    "input_fasta = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new.fasta\"\n",
    "output_fasta = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.fasta\"\n",
    "\n",
    "# Read all sequences from the input FASTA file\n",
    "all_sequences = list(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "\n",
    "# Randomly select 1 million sequences\n",
    "#num_sequences_to_select = 1600000\n",
    "num_sequences_to_select = 3000000\n",
    "\n",
    "subset_sequences = random.sample(all_sequences, min(num_sequences_to_select, len(all_sequences)))\n",
    "\n",
    "# Filter sequences to contain only A, T, G, and C, and have lengths as multiples of 3\n",
    "valid_sequences = [seq for seq in subset_sequences if set(str(seq.seq)).issubset(\"ATGC\")]\n",
    "\n",
    "# Ensure each selected sequence length is a multiple of 3 or remove 1 or 2 nucleotides\n",
    "for i in range(len(valid_sequences)):\n",
    "    seq_len = len(valid_sequences[i])\n",
    "    if seq_len % 3 != 0:\n",
    "        trim_length = seq_len % 3\n",
    "        valid_sequences[i] = valid_sequences[i][:-trim_length]\n",
    "\n",
    "# Write the selected sequences to the output FASTA file\n",
    "with open(output_fasta, \"w\") as output_handle:\n",
    "    SeqIO.write(valid_sequences, output_handle, \"fasta\")\n",
    "\n",
    "print(f\"Randomly selected {num_sequences_to_select} valid sequences and saved to {output_fasta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e64b91",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Statistics:\n",
      "Total Sequences: 1600000\n",
      "Total Bases: 1070400000\n",
      "Minimum Length: 669 bases\n",
      "Maximum Length: 669 bases\n",
      "Average Length: 669.00 bases\n"
     ]
    }
   ],
   "source": [
    "# Length Statistics for subset of filtered trimmed reads\n",
    "from Bio import SeqIO\n",
    "\n",
    "def generate_sequence_statistics(fasta_file):\n",
    "    sequence_lengths = []\n",
    "    total_sequences = 0\n",
    "    total_bases = 0\n",
    "\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            total_sequences += 1\n",
    "            sequence_length = len(record.seq)\n",
    "            sequence_lengths.append(sequence_length)\n",
    "            total_bases += sequence_length\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_length = min(sequence_lengths)\n",
    "    max_length = max(sequence_lengths)\n",
    "    average_length = total_bases / total_sequences\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"Sequence Statistics:\")\n",
    "    print(f\"Total Sequences: {total_sequences}\")\n",
    "    print(f\"Total Bases: {total_bases}\")\n",
    "    print(f\"Minimum Length: {min_length} bases\")\n",
    "    print(f\"Maximum Length: {max_length} bases\")\n",
    "    print(f\"Average Length: {average_length:.2f} bases\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fasta_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.fasta\"  # Replace with your FASTA file\n",
    "    generate_sequence_statistics(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c7dd77-502a-4f9f-acd6-dce74860234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Statistics:\n",
      "Total Sequences: 3000000\n",
      "Total Bases: 2007000000\n",
      "Minimum Length: 669 bases\n",
      "Maximum Length: 669 bases\n",
      "Average Length: 669.00 bases\n"
     ]
    }
   ],
   "source": [
    "# Length Statistics for filtered trimmed reads\n",
    "from Bio import SeqIO\n",
    "\n",
    "def generate_sequence_statistics(fasta_file):\n",
    "    sequence_lengths = []\n",
    "    total_sequences = 0\n",
    "    total_bases = 0\n",
    "\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            total_sequences += 1\n",
    "            sequence_length = len(record.seq)\n",
    "            sequence_lengths.append(sequence_length)\n",
    "            total_bases += sequence_length\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_length = min(sequence_lengths)\n",
    "    max_length = max(sequence_lengths)\n",
    "    average_length = total_bases / total_sequences\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"Sequence Statistics:\")\n",
    "    print(f\"Total Sequences: {total_sequences}\")\n",
    "    print(f\"Total Bases: {total_bases}\")\n",
    "    print(f\"Minimum Length: {min_length} bases\")\n",
    "    print(f\"Maximum Length: {max_length} bases\")\n",
    "    print(f\"Average Length: {average_length:.2f} bases\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fasta_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil.fasta\"  # Replace with your FASTA file\n",
    "    generate_sequence_statistics(fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140217c-1502-496a-ab10-658d92ae3490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7770759a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|██████████| 1600000/1600000 [15:45:45<00:00, 28.20it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 1599926\n",
      "Total sequences: 1600000\n",
      "Output CSV file saved as /mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.csv\n"
     ]
    }
   ],
   "source": [
    "# Script to write out matching fasta files with Accession ID values in metadata file to csv \n",
    "# files with Variant values from voc.json file and convert each nucleotide sequence to protein sequences\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count, Manager\n",
    "\n",
    "# Load your metadata DataFrame (df_metadata) and provide the correct path to your FASTA file and \"RBD.voc.json\" file.\n",
    "csv_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/metadata_2024_07_09.tsv\"\n",
    "df_metadata = pd.read_csv(csv_file, sep='\\t', dtype=str, low_memory=False, encoding='latin-1')\n",
    "\n",
    "# Load the voc.json file\n",
    "with open('/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/voc.json', 'r') as json_file:\n",
    "    voc_data = json.load(json_file)\n",
    "\n",
    "# Define a function to determine the VOC value based on Pango Lineage\n",
    "def determine_voc(lineage):\n",
    "    for voc, lineages in voc_data.items():\n",
    "        if lineage in lineages:\n",
    "            return voc\n",
    "    return \"nonVOC\"\n",
    "\n",
    "# Define the translation function\n",
    "def translate_nucleotides_to_protein(nucleotide_sequence):\n",
    "    return str(Seq(nucleotide_sequence).translate())\n",
    "\n",
    "# Define the output CSV file\n",
    "output_csv_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.csv\"\n",
    "\n",
    "# Initialize a count for matches\n",
    "match_count = 0\n",
    "total_sequences = 0\n",
    "\n",
    "# Function to process each sequence\n",
    "def process_sequence(record):\n",
    "    global df_metadata, voc_data\n",
    "    accession_id_fasta = record.description.split(\"|\")[1]\n",
    "    sequence = str(record.seq)\n",
    "\n",
    "    # Check if the Accession ID is present in the metadata DataFrame\n",
    "    match_row = df_metadata[df_metadata['Accession ID'] == accession_id_fasta]\n",
    "    if not match_row.empty:\n",
    "        lineage = match_row['Pango lineage'].values[0]\n",
    "\n",
    "        # Create a dictionary for the current record\n",
    "        output_data = {\n",
    "            'Accession ID': accession_id_fasta,\n",
    "            'Lineage': lineage,\n",
    "            'RBD nucleotide': sequence,\n",
    "            'Variant': determine_voc(lineage),\n",
    "            'RBD protein': translate_nucleotides_to_protein(sequence),\n",
    "        }\n",
    "        return output_data\n",
    "    return None\n",
    "\n",
    "# Write the header to the output CSV file\n",
    "with open(output_csv_file, 'w', newline='') as f:\n",
    "    writer = pd.DataFrame(columns=['Accession ID', 'Lineage', 'RBD nucleotide', 'Variant', 'RBD protein'])\n",
    "    writer.to_csv(f, index=False)\n",
    "\n",
    "# Estimate the total number of sequences for tqdm\n",
    "with open(\"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.fasta\", \"r\") as fasta_file:\n",
    "    total_sequences_estimated = sum(1 for line in fasta_file if line.startswith(\">\"))\n",
    "\n",
    "# Process sequences using multiprocessing Pool\n",
    "with Pool(processes = 32) as pool, tqdm(total=total_sequences_estimated, desc=\"Processing sequences\") as pbar:\n",
    "    for result in pool.imap(process_sequence, SeqIO.parse(\"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_new_1_6mil.fasta\", \"fasta\")):\n",
    "        pbar.update()\n",
    "        if result:\n",
    "            with open(output_csv_file, 'a', newline='') as f:\n",
    "                output_df = pd.DataFrame([result])\n",
    "                output_df.to_csv(f, mode='a', header=False, index=False)\n",
    "                match_count += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Matches found: {match_count}\")\n",
    "print(f\"Total sequences: {total_sequences_estimated}\")\n",
    "print(f\"Output CSV file saved as {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206dcaf1-4ac3-4fb6-a60f-96ce3638a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|██████████| 3000000/3000000 [05:05<00:00, 9805.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output CSV file saved as /mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Script to write out matching fasta files with Accession ID values in metadata file to csv \n",
    "# files with Variant values directly from metadata file \n",
    "# And convert each nucleotide sequence to protein sequences\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your metadata DataFrame and create a dictionary for quick lookups\n",
    "csv_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/metadata_2024_07_09.tsv\"\n",
    "df_metadata = pd.read_csv(csv_file, sep='\\t', dtype=str, low_memory=False, encoding='latin-1')\n",
    "\n",
    "# Create a dictionary with Accession ID as the key and (Variant, Pango lineage) as the value\n",
    "metadata_dict = df_metadata.set_index('Accession ID')[['Variant', 'Pango lineage']].to_dict('index')\n",
    "\n",
    "# Define the translation function\n",
    "def translate_nucleotides_to_protein(nucleotide_sequence):\n",
    "    return str(Seq(nucleotide_sequence).translate())\n",
    "\n",
    "# Define the output CSV file\n",
    "output_csv_file = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil.csv\"\n",
    "\n",
    "# Estimate the total number of sequences for tqdm\n",
    "with open(\"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil.fasta\", \"r\") as fasta_file:\n",
    "    total_sequences_estimated = sum(1 for line in fasta_file if line.startswith(\">\"))\n",
    "\n",
    "# Open the output CSV file for writing and write the header\n",
    "with open(output_csv_file, 'w', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['Accession ID', 'Lineage', 'RBD nucleotide', 'Variant', 'RBD protein'])\n",
    "\n",
    "    # Process sequences\n",
    "    with tqdm(total=total_sequences_estimated, desc=\"Processing sequences\") as pbar:\n",
    "        for record in SeqIO.parse(\"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil.fasta\", \"fasta\"):\n",
    "            accession_id_fasta = record.description.split(\"|\")[1]\n",
    "            sequence = str(record.seq)\n",
    "\n",
    "            # Check if the Accession ID is present in the metadata dictionary\n",
    "            if accession_id_fasta in metadata_dict:\n",
    "                metadata = metadata_dict[accession_id_fasta]\n",
    "                variant = metadata['Variant']\n",
    "                lineage = metadata['Pango lineage']\n",
    "\n",
    "                # Write the sequence data to the output CSV file\n",
    "                writer.writerow([\n",
    "                    accession_id_fasta,\n",
    "                    lineage,\n",
    "                    sequence,\n",
    "                    variant,\n",
    "                    translate_nucleotides_to_protein(sequence)\n",
    "                ])\n",
    "            pbar.update(1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Output CSV file saved as {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aeaadd7-0aba-4d1e-94bb-386fa4639921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved as /mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil_with_VOC.csv\n"
     ]
    }
   ],
   "source": [
    "# Mapping Variant label from input csv to respective recognizable VOC labels\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil.csv\"\n",
    "output_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/intermediate_files/filtered_msaCodon_1024_trimmed_RBD_3mil_with_VOC.csv\"\n",
    "\n",
    "# Mapping of keywords to VOC values\n",
    "variant_to_voc_mapping = {\n",
    "    'wuhanh1': 'WuhanHu1',\n",
    "    'delta': 'delta',\n",
    "    'omicron': 'omicron',\n",
    "    'alpha': 'alpha',\n",
    "    'beta': 'beta',\n",
    "    'epsilon': 'epsilon',\n",
    "    'eta': 'eta',\n",
    "    'iota': 'iota',\n",
    "    'kappa': 'kappa',\n",
    "    'mu': 'mu',\n",
    "    'lambda': 'lambda',\n",
    "    'gamma': 'gamma',\n",
    "    'zeta': 'zeta',\n",
    "}\n",
    "\n",
    "# Function to map Variant to VOC\n",
    "def map_variant_to_voc(variant):\n",
    "    if isinstance(variant, str):  # Check if the variant is a string\n",
    "        variant_lower = variant.lower()  # Convert to lowercase for case-insensitive matching\n",
    "        for keyword, voc_value in variant_to_voc_mapping.items():\n",
    "            if keyword in variant_lower:\n",
    "                return voc_value\n",
    "    return \"nonVOC\"  # Return a default value if no keyword matches or if variant is not a string\n",
    "\n",
    "# Load the input CSV file\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Apply the mapping function to create a new 'VOC' column\n",
    "df['VOC'] = df['Variant'].apply(map_variant_to_voc)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved as {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16821680-daca-4303-acf6-353b08a7f7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
