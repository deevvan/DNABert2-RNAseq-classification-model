{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36d2469-7207-4e66-8e15-4b1491d2ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences for each label:\n",
      "Label influenza_a_H3N2: 37081 sequences\n",
      "Label influenza_a_H1N1: 36966 sequences\n",
      "Label influenza_a_H5N1: 30288 sequences\n",
      "Label influenza_a_H1N2: 11402 sequences\n",
      "Label influenza_a_H9N2: 9766 sequences\n",
      "Label influenza_a_H5N8: 5809 sequences\n",
      "Label influenza_a_H7N9: 4648 sequences\n",
      "Label influenza_a_H3N8: 4622 sequences\n",
      "Label influenza_a_H5N6: 3846 sequences\n",
      "Label influenza_a_H4N6: 3190 sequences\n",
      "Label influenza_a_H5N2: 2719 sequences\n"
     ]
    }
   ],
   "source": [
    "# Extracting WGS or HA/NA segments from fasta files belonging to IAV strains\n",
    "# Removing sequences with ambigious nucleotides\n",
    "# Mapping IAV strain labels to numerical labels\n",
    "# Write out sequences, respective labels, segment name and EPI ID to output csv\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from glob import glob\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    if \"H1N1\" in filename:\n",
    "        return \"influenza_a_H1N1\"\n",
    "    elif \"H1N2\" in filename:\n",
    "        return \"influenza_a_H1N2\" \n",
    "    elif \"H3N2\" in filename:\n",
    "        return \"influenza_a_H3N2\"\n",
    "    elif \"H3N8\" in filename:\n",
    "        return \"influenza_a_H3N8\"\n",
    "    elif \"H4N6\" in filename:\n",
    "        return \"influenza_a_H4N6\"\n",
    "    elif \"H5N1\" in filename:\n",
    "        return \"influenza_a_H5N1\"\n",
    "    elif \"H5N2\" in filename:\n",
    "        return \"influenza_a_H5N2\"\n",
    "    elif \"H5N6\" in filename:\n",
    "        return \"influenza_a_H5N6\"\n",
    "    elif \"H5N8\" in filename:\n",
    "        return \"influenza_a_H5N8\"\n",
    "    elif \"H7N9\" in filename:\n",
    "        return \"influenza_a_H7N9\"\n",
    "    elif \"H9N2\" in filename:\n",
    "        return \"influenza_a_H9N2\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_fasta_file(fasta_file):\n",
    "    valid_nucleotides = {'A', 'T', 'G', 'C'}\n",
    "    label = get_label_from_filename(fasta_file)\n",
    "    if label is None:\n",
    "        return []\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = ''\n",
    "        epi_id = ''\n",
    "        segment = ''\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                if sequence and set(sequence.upper()).issubset(valid_nucleotides):\n",
    "                    sequences.append((sequence.upper(), epi_id, segment, label))\n",
    "                header_parts = line.strip().split('|')\n",
    "                epi_id = header_parts[1] if len(header_parts) > 1 else ''\n",
    "                segment = header_parts[6] if len(header_parts) > 6 else ''\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        \n",
    "        if sequence and set(sequence.upper()).issubset(valid_nucleotides):\n",
    "            sequences.append((sequence.upper(), epi_id, segment, label))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def label_to_number(label):\n",
    "    label_map = {\n",
    "        \"influenza_a_H1N1\": 1,\n",
    "        \"influenza_a_H1N2\": 2,\n",
    "        \"influenza_a_H3N2\": 3,\n",
    "        \"influenza_a_H3N8\": 4,\n",
    "        \"influenza_a_H4N6\": 5,\n",
    "        \"influenza_a_H5N1\": 6,\n",
    "        \"influenza_a_H5N2\": 7,\n",
    "        \"influenza_a_H5N6\": 8,\n",
    "        \"influenza_a_H5N8\": 9,\n",
    "        \"influenza_a_H7N9\": 10,\n",
    "        \"influenza_a_H9N2\": 11\n",
    "    }\n",
    "    return label_map.get(label, 0)\n",
    "\n",
    "def process_fasta_files(directory, output_csv):\n",
    "    fasta_files = glob(os.path.join(directory, \"*.fasta\"))\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(process_fasta_file, fasta_files)\n",
    "\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['sequence', 'EPI_ID', 'segment', 'label_name', 'label_number']\n",
    "        df = pd.DataFrame(columns=fieldnames)\n",
    "        df.to_csv(csv_file, mode='w', index=False, header=True)\n",
    "\n",
    "        for result in results:\n",
    "            for sequence, epi_id, segment, label in result:\n",
    "                label_number = label_to_number(label)\n",
    "                df = pd.DataFrame([[sequence, epi_id, segment, label, label_number]], columns=fieldnames)\n",
    "                df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "    # Load the CSV and count the number of sequences for each label\n",
    "    data = pd.read_csv(output_csv)\n",
    "    label_counts = data['label_name'].value_counts()\n",
    "\n",
    "    print(\"Number of sequences for each label:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} sequences\")\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/IAV_WGS'\n",
    "#main_dir = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/IAV_strains'\n",
    "\n",
    "process_fasta_files(main_dir, 'WGS_IAV_strains.csv')\n",
    "#process_fasta_files(main_dir, 'HA_NA_IAV_strains.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0b753-9596-4989-9b4b-b3c50cc44fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f318993-d9a9-41c4-8b9f-39abb40b5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [00:58,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragments and their reverse complements have been written to /mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/intermediate_csvs/HA_NA_IAV_strains_250bp_50overlap_complementary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating 250bps fragments with 50 bp overlaps from WGS or HA/NA segments generated above\n",
    "# Generating reverse complementary sequence for each generated fragment \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Function to generate overlapping 250 bp fragments\n",
    "def generate_overlapping_fragments(sequence, fragment_len=250, overlap=200):\n",
    "    fragments = []\n",
    "    for start in range(0, len(sequence) - fragment_len + 1, fragment_len - overlap):\n",
    "        fragment = sequence[start:start + fragment_len]\n",
    "        if len(fragment) == fragment_len:\n",
    "            fragments.append(fragment)\n",
    "    return fragments\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/\"\n",
    "intermediate_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/intermediate_csvs\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "#input_csv_path = os.path.join(\"HA_NA_IAV_strains.csv\")\n",
    "input_csv_path = os.path.join(\"WGS_IAV_strains.csv\")\n",
    "\n",
    "#output_csv_path = os.path.join(intermediate_dir, \"HA_NA_IAV_strains_250bp_50overlap_complementary.csv\")\n",
    "output_csv_path = os.path.join(intermediate_dir, \"WGS_IAV_strains_250bp_50overlap_complementary.csv\")\n",
    "\n",
    "# Read the input CSV file in chunks\n",
    "chunksize = 1000  # Adjust the chunk size as needed\n",
    "input_columns = [\"EPI_ID\", \"segment\",\n",
    "                 \"label_name\", \"label_number\", \n",
    "                 \"sequence\"]\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"EPI_ID\", \"segment\",\n",
    "                     \"label_name\", \"label_number\",  \n",
    "                     \"sequence\"])  # Write header\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(input_csv_path, usecols=input_columns, chunksize=chunksize)):\n",
    "        for index, row in chunk.iterrows():\n",
    "            epi_id = row[\"EPI_ID\"]\n",
    "            segment = row[\"segment\"]\n",
    "            variant_label = row[\"label_name\"]\n",
    "            variant_label_number = row[\"label_number\"]\n",
    "            sequence = row[\"sequence\"]\n",
    "            \n",
    "            fragments = generate_overlapping_fragments(sequence)\n",
    "            \n",
    "            for fragment in fragments:\n",
    "                # Write the original fragment\n",
    "                writer.writerow([epi_id, segment, variant_label, variant_label_number, fragment])\n",
    "                \n",
    "                # Generate the reverse complementary fragment\n",
    "                reverse_complement_fragment = str(Seq(fragment).reverse_complement())\n",
    "                \n",
    "                # Write the reverse complementary fragment\n",
    "                writer.writerow([epi_id, segment, variant_label, variant_label_number, reverse_complement_fragment])\n",
    "\n",
    "print(f\"Fragments and their reverse complements have been written to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99150993-2c35-47fc-9f8e-6d378387c7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAV labels: influenza_a_H1N1, Unique EPI_ID Count: 4280\n",
      "IAV labels: influenza_a_H1N2, Unique EPI_ID Count: 3441\n",
      "IAV labels: influenza_a_H3N2, Unique EPI_ID Count: 7184\n",
      "IAV labels: influenza_a_H3N8, Unique EPI_ID Count: 2366\n",
      "IAV labels: influenza_a_H4N6, Unique EPI_ID Count: 1772\n",
      "IAV labels: influenza_a_H5N1, Unique EPI_ID Count: 7016\n",
      "IAV labels: influenza_a_H7N9, Unique EPI_ID Count: 2498\n",
      "IAV labels: influenza_a_H9N2, Unique EPI_ID Count: 5538\n"
     ]
    }
   ],
   "source": [
    "# Calculating the number of unique EPI IDs for each unique value of label_name (IAV strain)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the CSV file path is 'WGS_by_VOC_finetune.csv'\n",
    "iav_csv_file_path = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/intermediate_csvs/WGS_IAV_strains_250bp_50overlap_complementary.csv'\n",
    "#iav_csv_file_path = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/intermediate_csvs/HA_NA_IAV_strains_250bp_50overlap_complementary.csv'\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "data_iav = pd.read_csv(iav_csv_file_path)\n",
    "\n",
    "# Group by label_name and count unique EPI_ID values\n",
    "label_counts_iav = data_iav.groupby('label_name')['EPI_ID'].nunique()\n",
    "\n",
    "# Print out the counts\n",
    "for label, count in label_counts_iav.items():\n",
    "    print(f\"IAV labels: {label}, Unique EPI_ID Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a957bed-026d-43f7-9a41-3d330e679293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences and unique EPI ID values for each label:\n",
      "Label influenza_a_H1N1: 1360680 sequences, 3000 unique EPI IDs\n",
      "Label influenza_a_H1N2: 1297556 sequences, 3000 unique EPI IDs\n",
      "Label influenza_a_H3N2: 1324690 sequences, 3000 unique EPI IDs\n",
      "Label influenza_a_H5N1: 1270086 sequences, 3000 unique EPI IDs\n",
      "Label influenza_a_H9N2: 1247340 sequences, 3000 unique EPI IDs\n"
     ]
    }
   ],
   "source": [
    "# Creating subset of above output csv with desired number of unique EPI_IDs for each IAV strain label\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def filter_sequences(input_csv, output_csv):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(input_csv,low_memory=False)\n",
    "    \n",
    "    # Count unique EPI IDs per label_name\n",
    "    unique_epi_counts = data.groupby('label_name')['EPI_ID'].nunique()\n",
    "    \n",
    "    # Find labels with at least 100 unique EPI IDs\n",
    "    valid_labels = unique_epi_counts[unique_epi_counts >= 3000].index\n",
    "    \n",
    "    # Filter the dataset to include only the valid labels\n",
    "    filtered_data = data[data['label_name'].isin(valid_labels)]\n",
    "    \n",
    "    # Initialize a list to store the filtered sequences\n",
    "    filtered_sequences = []\n",
    "    \n",
    "    # For each valid label, select 5k random EPI IDs and filter sequences\n",
    "    for label in valid_labels:\n",
    "        # Get unique EPI IDs for the current label\n",
    "        label_data = filtered_data[filtered_data['label_name'] == label]\n",
    "        unique_ep_ids = label_data['EPI_ID'].unique()\n",
    "        \n",
    "        # Randomly select 5k unique EPI IDs\n",
    "        if len(unique_ep_ids) > 3000:\n",
    "            selected_ep_ids = pd.Series(unique_ep_ids).sample(n=3000, random_state=1).tolist()\n",
    "        else:\n",
    "            selected_ep_ids = unique_ep_ids\n",
    "        \n",
    "        # Filter data based on selected EPI IDs\n",
    "        selected_data = label_data[label_data['EPI_ID'].isin(selected_ep_ids)]\n",
    "        filtered_sequences.append(selected_data)\n",
    "    \n",
    "    # Concatenate all filtered sequences into a single DataFrame\n",
    "    result_df = pd.concat(filtered_sequences)\n",
    "    \n",
    "    # Save the filtered data to a new CSV file\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print number of sequences and unique EPI ID values for each label_name\n",
    "    filtered_label_counts = result_df.groupby('label_name').agg(\n",
    "        num_sequences=('sequence', 'count'),\n",
    "        num_unique_epi_ids=('EPI_ID', 'nunique')\n",
    "    )\n",
    "    \n",
    "    print(\"Number of sequences and unique EPI ID values for each label:\")\n",
    "    for label, row in filtered_label_counts.iterrows():\n",
    "        print(f\"Label {label}: {row['num_sequences']} sequences, {row['num_unique_epi_ids']} unique EPI IDs\")\n",
    "\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/\"\n",
    "intermediate_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/intermediate_csvs\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv = os.path.join(intermediate_dir, \"WGS_IAV_strains_250bp_50overlap_complementary.csv\")\n",
    "#input_csv = os.path.join(intermediate_dir, \"HA_NA_IAV_strains_250bp_50overlap_complementary.csv\")\n",
    "\n",
    "output_csv = os.path.join(main_dir, \"WGS_IAV_strains_250bp_50overlap_complementary_3k_epi.csv\")\n",
    "#output_csv = os.path.join(main_dir, \"HA_NA_IAV_strains_250bp_50overlap_complementary_500_epi.csv\")\n",
    "\n",
    "# Execute the filtering and reporting\n",
    "filter_sequences(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb9736c-0b0d-4855-b5bd-7d6861a7b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Name: influenza_a_H1N1\n",
      "  - Unique Label Numbers: [1]\n",
      "  - Number of Unique EPI_IDs: 3000\n",
      "----------------------------------------\n",
      "Label Name: influenza_a_H1N2\n",
      "  - Unique Label Numbers: [2]\n",
      "  - Number of Unique EPI_IDs: 3000\n",
      "----------------------------------------\n",
      "Label Name: influenza_a_H3N2\n",
      "  - Unique Label Numbers: [3]\n",
      "  - Number of Unique EPI_IDs: 3000\n",
      "----------------------------------------\n",
      "Label Name: influenza_a_H5N1\n",
      "  - Unique Label Numbers: [6]\n",
      "  - Number of Unique EPI_IDs: 3000\n",
      "----------------------------------------\n",
      "Label Name: influenza_a_H9N2\n",
      "  - Unique Label Numbers: [11]\n",
      "  - Number of Unique EPI_IDs: 3000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if label_names are correctly mapped to label_numbers\n",
    "import pandas as pd\n",
    "\n",
    "def check_label_numbers_and_count_epi_ids(input_csv):\n",
    "    \"\"\"\n",
    "    Check the label_number values for each unique label_name value and print the number of EPI_ID values for each label_name.\n",
    "    \"\"\"\n",
    "    # Load the input CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Group by label_name and analyze label_number and EPI_ID counts\n",
    "    grouped = df.groupby('label_name')\n",
    "\n",
    "    for label_name, group in grouped:\n",
    "        unique_label_numbers = group['label_number'].unique()\n",
    "        epi_id_count = group['EPI_ID'].nunique()  # Count unique EPI_ID values\n",
    "\n",
    "        print(f\"Label Name: {label_name}\")\n",
    "        print(f\"  - Unique Label Numbers: {unique_label_numbers}\")\n",
    "        print(f\"  - Number of Unique EPI_IDs: {epi_id_count}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "input_csv = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_IAV_strains_250bp_50overlap_complementary_3k_epi.csv\"\n",
    "#input_csv = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/HA_NA_IAV_strains_250bp_50overlap_complementary_500_epi.csv\"\n",
    "\n",
    "# Run the function\n",
    "check_label_numbers_and_count_epi_ids(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3117c39-0e70-46b1-bb1a-6ac536b748aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAV labels: influenza_a_H1N1, Unique EPI_ID Count: 3000\n",
      "IAV labels: influenza_a_H1N2, Unique EPI_ID Count: 3000\n",
      "IAV labels: influenza_a_H3N2, Unique EPI_ID Count: 3000\n",
      "IAV labels: influenza_a_H5N1, Unique EPI_ID Count: 3000\n",
      "IAV labels: influenza_a_H9N2, Unique EPI_ID Count: 3000\n"
     ]
    }
   ],
   "source": [
    "# Verifying the number of unique EPI IDs for each IAV strain label\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the CSV file path is 'WGS_by_VOC_finetune.csv'\n",
    "iav_csv_file_path = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_IAV_strains_250bp_50overlap_complementary_3k_epi.csv'\n",
    "# Load the CSV file\n",
    "data_iav = pd.read_csv(iav_csv_file_path)\n",
    "\n",
    "# Group by label_name and count unique EPI_ID values\n",
    "label_counts_iav = data_iav.groupby('label_name')['EPI_ID'].nunique()\n",
    "\n",
    "# Print out the counts\n",
    "for label, count in label_counts_iav.items():\n",
    "    print(f\"IAV labels: {label}, Unique EPI_ID Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851c3cc-0b14-4409-add0-68a6ae685222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
