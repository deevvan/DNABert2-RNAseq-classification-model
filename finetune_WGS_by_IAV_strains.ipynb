{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36d2469-7207-4e66-8e15-4b1491d2ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the main directory path:  /mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/variant_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences for each label:\n",
      "Label influenza_a_H3N2: 37081 sequences\n",
      "Label influenza_a_H1N1: 36966 sequences\n",
      "Label sars_cov_2_alpha: 10000 sequences\n",
      "Label sars_cov_2_beta: 10000 sequences\n",
      "Label sars_cov_2_delta: 10000 sequences\n",
      "Label sars_cov_2_gamma: 10000 sequences\n",
      "Label sars_cov_2_omicron: 10000 sequences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    if \"alpha\" in filename:\n",
    "        return \"sars_cov_2_alpha\"\n",
    "    elif \"beta\" in filename:\n",
    "        return \"sars_cov_2_beta\"\n",
    "    elif \"gamma\" in filename:\n",
    "        return \"sars_cov_2_gamma\"\n",
    "    elif \"delta\" in filename:\n",
    "        return \"sars_cov_2_delta\"\n",
    "    elif \"omicron\" in filename:\n",
    "        return \"sars_cov_2_omicron\"\n",
    "    elif \"influenzaA_H1N1\" in filename:\n",
    "        return \"influenza_a_H1N1\"\n",
    "    elif \"influenzaA_H3N2\" in filename:\n",
    "        return \"influenza_a_H3N2\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_fasta_file(fasta_file):\n",
    "    valid_nucleotides = {'A', 'T', 'G', 'C'}\n",
    "    label = get_label_from_filename(fasta_file)\n",
    "    if label is None:\n",
    "        return []\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = ''\n",
    "        epi_id = ''\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                if sequence and set(sequence.upper()).issubset(valid_nucleotides):\n",
    "                    sequences.append((sequence.upper(), epi_id, label))\n",
    "                header_parts = line.strip().split('|')\n",
    "                if \"hcov-19\" in fasta_file:\n",
    "                    epi_id = header_parts[3] if len(header_parts) > 3 else ''\n",
    "                elif \"influenzaA\" in fasta_file:\n",
    "                    epi_id = header_parts[1] if len(header_parts) > 1 else ''\n",
    "                sequence = ''\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        \n",
    "        if sequence and set(sequence.upper()).issubset(valid_nucleotides):\n",
    "            sequences.append((sequence.upper(), epi_id, label))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def label_to_number(label):\n",
    "    label_map = {\n",
    "        \"sars_cov_2_alpha\": 1,\n",
    "        \"sars_cov_2_beta\": 2,\n",
    "        \"sars_cov_2_gamma\": 3,\n",
    "        \"sars_cov_2_delta\": 4,\n",
    "        \"sars_cov_2_omicron\": 5,\n",
    "        \"influenza_a_H1N1\": 6,\n",
    "        \"influenza_a_H3N2\": 7\n",
    "    }\n",
    "    return label_map.get(label, 0)\n",
    "\n",
    "def process_fasta_files(directory, output_csv):\n",
    "    fasta_files = os.popen(f\"ls {directory}/*.fasta\").read().split()\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(process_fasta_file, fasta_files)\n",
    "\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['sequence', 'EPI_ID', 'label_name', 'label_number']\n",
    "        df = pd.DataFrame(columns=fieldnames)\n",
    "        df.to_csv(csv_file, mode='w', index=False, header=True)\n",
    "\n",
    "        for result in results:\n",
    "            for sequence, epi_id, label in result:\n",
    "                label_number = label_to_number(label)\n",
    "                df = pd.DataFrame([[sequence, epi_id, label, label_number]], columns=fieldnames)\n",
    "                df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "    # Load the CSV and count the number of sequences for each label\n",
    "    data = pd.read_csv(output_csv)\n",
    "    label_counts = data['label_name'].value_counts()\n",
    "\n",
    "    print(\"Number of sequences for each label:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} sequences\")\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = input(\"Enter the main directory path to fasta files for variants/strains: \")\n",
    "process_fasta_files(main_dir, 'WGS_by_VOC_finetune.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0b753-9596-4989-9b4b-b3c50cc44fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9bd0bf-d99b-4fdf-8cb3-d9ca4380b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting completed. Files saved to /mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_hcov19_finetune.csv and /mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_IAV_finetune.csv.\n"
     ]
    }
   ],
   "source": [
    "## Separating voc csv file into two files based on virus name to train a model for each virus separately:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_finetune.csv\"\n",
    "output_csv_sars_cov2_path = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_hcov19_finetune.csv\"\n",
    "output_csv_influenza_a_path = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_IAV_finetune.csv\"\n",
    "\n",
    "# Create file handlers for output CSV files\n",
    "with open(output_csv_sars_cov2_path, 'w') as sars_cov2_file, open(output_csv_influenza_a_path, 'w') as influenza_a_file:\n",
    "    # Read the header line from the input CSV file\n",
    "    with open(input_csv_path, 'r') as file:\n",
    "        header = file.readline().strip()\n",
    "        sars_cov2_file.write(header + '\\n')\n",
    "        influenza_a_file.write(header + '\\n')\n",
    "    \n",
    "    # Read and process the input CSV file in chunks\n",
    "    chunk_size = 10000  # Adjust chunk size based on memory constraints\n",
    "    for chunk in pd.read_csv(input_csv_path, chunksize=chunk_size, low_memory=False):\n",
    "        # Identify rows for sars_cov2 and influenza_a\n",
    "        sars_cov2_rows = chunk[chunk['label_name'].str.contains('sars_cov_2', na=False)]\n",
    "        influenza_a_rows = chunk[chunk['label_name'].str.contains('influenza_a', na=False)]\n",
    "        \n",
    "        # Write rows to respective CSV files\n",
    "        sars_cov2_rows.to_csv(sars_cov2_file, mode='a', header=False, index=False)\n",
    "        influenza_a_rows.to_csv(influenza_a_file, mode='a', header=False, index=False)\n",
    "    \n",
    "print(f\"Splitting completed. Files saved to {output_csv_sars_cov2_path} and {output_csv_influenza_a_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f318993-d9a9-41c4-8b9f-39abb40b5b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a957bed-026d-43f7-9a41-3d330e679293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences and unique EPI ID values for each label:\n",
      "Label sars_cov_2_alpha: 2000 sequences, 2000 unique EPI IDs\n",
      "Label sars_cov_2_beta: 2000 sequences, 2000 unique EPI IDs\n",
      "Label sars_cov_2_delta: 2000 sequences, 2000 unique EPI IDs\n",
      "Label sars_cov_2_gamma: 2000 sequences, 2000 unique EPI IDs\n",
      "Label sars_cov_2_omicron: 2000 sequences, 2000 unique EPI IDs\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for the two virus' variants with 5k unique EPI_IDs\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def filter_sequences(input_csv, output_csv):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(input_csv,low_memory=False)\n",
    "    \n",
    "    # Count unique EPI IDs per label_name\n",
    "    unique_epi_counts = data.groupby('label_name')['EPI_ID'].nunique()\n",
    "    \n",
    "    # Find labels with at least 100 unique EPI IDs\n",
    "    valid_labels = unique_epi_counts[unique_epi_counts >= 5000].index\n",
    "    \n",
    "    # Filter the dataset to include only the valid labels\n",
    "    filtered_data = data[data['label_name'].isin(valid_labels)]\n",
    "    \n",
    "    # Initialize a list to store the filtered sequences\n",
    "    filtered_sequences = []\n",
    "    \n",
    "    # For each valid label, select 5k random EPI IDs and filter sequences\n",
    "    for label in valid_labels:\n",
    "        # Get unique EPI IDs for the current label\n",
    "        label_data = filtered_data[filtered_data['label_name'] == label]\n",
    "        unique_ep_ids = label_data['EPI_ID'].unique()\n",
    "        \n",
    "        # Randomly select 5k unique EPI IDs\n",
    "        if len(unique_ep_ids) > 5000:\n",
    "            selected_ep_ids = pd.Series(unique_ep_ids).sample(n=5000, random_state=1).tolist()\n",
    "        else:\n",
    "            selected_ep_ids = unique_ep_ids\n",
    "        \n",
    "        # Filter data based on selected EPI IDs\n",
    "        selected_data = label_data[label_data['EPI_ID'].isin(selected_ep_ids)]\n",
    "        filtered_sequences.append(selected_data)\n",
    "    \n",
    "    # Concatenate all filtered sequences into a single DataFrame\n",
    "    result_df = pd.concat(filtered_sequences)\n",
    "    \n",
    "    # Save the filtered data to a new CSV file\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print number of sequences and unique EPI ID values for each label_name\n",
    "    filtered_label_counts = result_df.groupby('label_name').agg(\n",
    "        num_sequences=('sequence', 'count'),\n",
    "        num_unique_epi_ids=('EPI_ID', 'nunique')\n",
    "    )\n",
    "    \n",
    "    print(\"Number of sequences and unique EPI ID values for each label:\")\n",
    "    for label, row in filtered_label_counts.iterrows():\n",
    "        print(f\"Label {label}: {row['num_sequences']} sequences, {row['num_unique_epi_ids']} unique EPI IDs\")\n",
    "\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "# Assuming the CSV file path is 'WGS_by_VOC_finetune.csv'\n",
    "#input_csv = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_hcov19_finetune.csv'\n",
    "input_csv = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_IAV_finetune.csv'\n",
    "\n",
    "#output_csv = os.path.join(main_dir, \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_hcov19_finetune_5k_epi.csv\")\n",
    "output_csv = os.path.join(main_dir, \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_IAV_finetune_5k_epi.csv\")\n",
    "\n",
    "# Execute the filtering and reporting\n",
    "filter_sequences(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9736c-0b0d-4855-b5bd-7d6861a7b093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6f9a0e-d518-44d4-a613-1cfd7976de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:03,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating overlapping 250bps fragments with labels for each one of the two hcov19 5k sequences and IAV 5k sequences \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate overlapping 250 bp fragments\n",
    "def generate_overlapping_fragments(sequence, fragment_len=250, overlap=200):\n",
    "    fragments = []\n",
    "    for start in range(0, len(sequence) - fragment_len + 1, fragment_len - overlap):\n",
    "        fragment = sequence[start:start + fragment_len]\n",
    "        if len(fragment) == fragment_len:\n",
    "            fragments.append(fragment)\n",
    "    return fragments\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "main_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv_path = os.path.join(main_dir, \"WGS_by_VOC_hcov19_finetune_2k_epi.csv\")\n",
    "#input_csv_path = os.path.join(main_dir, \"WGS_by_VOC_IAV_finetune_5k_epi.csv\")\n",
    "\n",
    "output_csv_path = os.path.join(main_dir, \"WGS_by_VOC_hcov19_finetune_2k_epi_250bp_200overlap.csv\")\n",
    "#output_csv_path = os.path.join(main_dir, \"WGS_by_VOC_IAV_finetune_5k_epi_250bp_fragments.csv\")\n",
    "\n",
    "# Read the input CSV file in chunks\n",
    "chunksize = 1000  # Adjust the chunk size as needed\n",
    "input_columns = [\"EPI_ID\", \n",
    "                 \"label_name\", \"label_number\", \n",
    "                 \"sequence\"]\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"EPI_ID\", \n",
    "                     \"label_name\", \"label_number\",  \n",
    "                     \"sequence\"])  # Write header\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(input_csv_path, usecols=input_columns, chunksize=chunksize)):\n",
    "        for index, row in chunk.iterrows():\n",
    "            epi_id = row[\"EPI_ID\"]\n",
    "            variant_label = row[\"label_name\"]\n",
    "            variant_label_number = row[\"label_number\"]\n",
    "            sequence = row[\"sequence\"]\n",
    "            \n",
    "            fragments = generate_overlapping_fragments(sequence)\n",
    "            \n",
    "            for fragment in fragments:\n",
    "                writer.writerow([epi_id, variant_label, variant_label_number, fragment])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3117c39-0e70-46b1-bb1a-6ac536b748aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f19a7aeb-bc0b-43f2-b84f-15fcc72504c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARS-CoV-2 labels: sars_cov_2_alpha, Unique EPI_ID Count: 2000\n",
      "SARS-CoV-2 labels: sars_cov_2_beta, Unique EPI_ID Count: 2000\n",
      "SARS-CoV-2 labels: sars_cov_2_delta, Unique EPI_ID Count: 2000\n",
      "SARS-CoV-2 labels: sars_cov_2_gamma, Unique EPI_ID Count: 2000\n",
      "SARS-CoV-2 labels: sars_cov_2_omicron, Unique EPI_ID Count: 2000\n",
      "IAV labels: influenza_a_H1N1, Unique EPI_ID Count: 2000\n",
      "IAV labels: influenza_a_H3N2, Unique EPI_ID Count: 2000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique EPI IDs for each unique value of label_name\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the CSV file path is 'WGS_by_VOC_finetune.csv'\n",
    "cov_csv_file_path = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_hcov19_finetune_2k_epi_250bp_fragments.csv'\n",
    "iav_csv_file_path = '/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/WGS_by_VOC_IAV_finetune_2k_epi_250bp_fragments.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "data_cov = pd.read_csv(cov_csv_file_path)\n",
    "data_iav = pd.read_csv(iav_csv_file_path)\n",
    "\n",
    "# Group by label_name and count unique EPI_ID values\n",
    "label_counts_cov = data_cov.groupby('label_name')['EPI_ID'].nunique()\n",
    "label_counts_iav = data_iav.groupby('label_name')['EPI_ID'].nunique()\n",
    "\n",
    "# Print out the counts\n",
    "for label, count in label_counts_cov.items():\n",
    "    print(f\"SARS-CoV-2 labels: {label}, Unique EPI_ID Count: {count}\")\n",
    "for label, count in label_counts_iav.items():\n",
    "    print(f\"IAV labels: {label}, Unique EPI_ID Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851c3cc-0b14-4409-add0-68a6ae685222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
