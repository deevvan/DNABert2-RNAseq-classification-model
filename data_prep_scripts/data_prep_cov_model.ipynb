{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a68598-695d-48ee-b0cd-2eec4b6adc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/gisaid_data/main_MSA_files/filtered_msaCodon_1024_trimmed_RBD_3mil_with_VOC.csv\"\n",
    "input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/filtered_msaCodon_1024_trimmed_RBD_15mil.csv\"\n",
    "in_df=pd.read_csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aaf8027-a0e8-4b77-b3ad-0bdacbc4f1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accession ID</th>\n",
       "      <th>Lineage</th>\n",
       "      <th>RBD nucleotide</th>\n",
       "      <th>Variant</th>\n",
       "      <th>VOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPI_ISL_10839200</td>\n",
       "      <td>BA.1.15</td>\n",
       "      <td>ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...</td>\n",
       "      <td>Former VOC Omicron GRA (B.1.1.529+BA.*)</td>\n",
       "      <td>Omicron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPI_ISL_5652980</td>\n",
       "      <td>P.1.11</td>\n",
       "      <td>ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...</td>\n",
       "      <td>Former VOC Gamma GR/501Y.V3 (P.1+P.1.*)</td>\n",
       "      <td>Gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPI_ISL_17397934</td>\n",
       "      <td>XBB.1.9.2</td>\n",
       "      <td>ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...</td>\n",
       "      <td>VUM GRA (XBB.1.9.2+XBB1.9.2.*)</td>\n",
       "      <td>VUM_GRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPI_ISL_9443030</td>\n",
       "      <td>BA.1</td>\n",
       "      <td>ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...</td>\n",
       "      <td>Former VOC Omicron GRA (B.1.1.529+BA.*)</td>\n",
       "      <td>Omicron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPI_ISL_4353158</td>\n",
       "      <td>AY.4</td>\n",
       "      <td>ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...</td>\n",
       "      <td>Former VOC Delta GK (B.1.617.2+AY.*)</td>\n",
       "      <td>Delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accession ID    Lineage  \\\n",
       "0  EPI_ISL_10839200    BA.1.15   \n",
       "1   EPI_ISL_5652980     P.1.11   \n",
       "2  EPI_ISL_17397934  XBB.1.9.2   \n",
       "3   EPI_ISL_9443030       BA.1   \n",
       "4   EPI_ISL_4353158       AY.4   \n",
       "\n",
       "                                      RBD nucleotide  \\\n",
       "0  ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...   \n",
       "1  ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...   \n",
       "2  ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...   \n",
       "3  ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...   \n",
       "4  ACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACT...   \n",
       "\n",
       "                                   Variant      VOC  \n",
       "0  Former VOC Omicron GRA (B.1.1.529+BA.*)  Omicron  \n",
       "1  Former VOC Gamma GR/501Y.V3 (P.1+P.1.*)    Gamma  \n",
       "2           VUM GRA (XBB.1.9.2+XBB1.9.2.*)  VUM_GRA  \n",
       "3  Former VOC Omicron GRA (B.1.1.529+BA.*)  Omicron  \n",
       "4     Former VOC Delta GK (B.1.617.2+AY.*)    Delta  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f176e4-7e93-4f29-838e-22cd44663fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC\n",
      "Omicron    5450788\n",
      "Delta      3888578\n",
      "Alpha      1043401\n",
      "nonVOC      940219\n",
      "VUM_GRA     269553\n",
      "Eris        167378\n",
      "Gamma       113831\n",
      "Epsilon      58365\n",
      "Iota         37878\n",
      "Beta         14825\n",
      "Mu           11307\n",
      "Lambda        9169\n",
      "Eta           7079\n",
      "Zeta          6188\n",
      "FLiRT         4277\n",
      "Kappa         3594\n",
      "VUM_GH         805\n",
      "VOI_GRA        207\n",
      "Theta           85\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "voc_counts = in_df['VOC'].value_counts()\n",
    "print (voc_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fce43ab-b4ee-475c-9599-99699bf91cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of label names to numeric values:\n",
      "sars_cov_2_Omicron: 0\n",
      "sars_cov_2_Gamma: 1\n",
      "sars_cov_2_Delta: 2\n",
      "sars_cov_2_Alpha: 3\n",
      "sars_cov_2_Eris: 4\n",
      "sars_cov_2_Lambda: 5\n",
      "sars_cov_2_Zeta: 6\n",
      "sars_cov_2_Kappa: 7\n",
      "sars_cov_2_Beta: 8\n",
      "sars_cov_2_Mu: 9\n",
      "sars_cov_2_Eta: 10\n",
      "sars_cov_2_FLiRT: 11\n",
      "sars_cov_2_Theta: 12\n"
     ]
    }
   ],
   "source": [
    "# Path2) Use Variant column value from metadata to extract Variant label\n",
    "# Creating uniform variant label values from trimmed RBD dataset and mapping labels to numeric values\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def transform_csv(input_csv, output_csv):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Create the new 'label_name' column by prefixing 'sars_cov_2_' to the 'VOC' column\n",
    "    df['label_name'] = 'sars_cov_2_' + df['VOC']\n",
    "\n",
    "    # Remove rows where 'label_name' is 'sars_cov_2_nonVOC'\n",
    "    df = df[df['label_name'] != 'sars_cov_2_nonVOC']\n",
    "    \n",
    "    # Removing VOI and VUM labels\n",
    "    df = df[df['label_name'] != 'sars_cov_2_VUM_GRA']\n",
    "    df = df[df['label_name'] != 'sars_cov_2_VUM_GH']\n",
    "    df = df[df['label_name'] != 'sars_cov_2_VOI_GRA']\n",
    "    df = df[df['label_name'] != 'sars_cov_2_Iota']\n",
    "    df = df[df['label_name'] != 'sars_cov_2_Epsilon']\n",
    "    \n",
    "\n",
    "    # Create a mapping from unique 'label_name' to a numeric value\n",
    "    label_mapping = {label: idx for idx, label in enumerate(df['label_name'].unique())}\n",
    "\n",
    "    # Create the 'label_number' column by mapping 'label_name' to its corresponding numeric value\n",
    "    df['label_number'] = df['label_name'].map(label_mapping)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'Accession ID': 'EPI_ID', 'RBD nucleotide': 'sequence'})\n",
    "\n",
    "    # Select relevant columns and write them to the output CSV file\n",
    "    df[['EPI_ID', 'sequence', 'label_name', 'label_number']].to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print the mapping of label names to numbers\n",
    "    print(\"Mapping of label names to numeric values:\")\n",
    "    for label, number in label_mapping.items():\n",
    "        print(f\"{label}: {number}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/filtered_msaCodon_1024_trimmed_RBD_15mil.csv\"\n",
    "    output_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/RBD_nucleotides_15mil_wo_nonvoc.csv\"\n",
    "    transform_csv(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f9601f-764f-4a31-80f4-556e1ba79899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'N' found in the 'RBD nucleotide' column.\n"
     ]
    }
   ],
   "source": [
    "# Checking for ambigious nucleotides\n",
    "import pandas as pd\n",
    "#rbd_df= pd.read_csv('/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/RBD_nucleotides_15mil_wo_nonvoc.csv')\n",
    "\n",
    "# Look for presence of ambigious nucleotide N in any sequence\n",
    "if rbd_df['sequence'].str.contains('N').any():\n",
    "    print(\"There is at least one 'N' in the 'RBD nucleotide' column.\")\n",
    "else:\n",
    "    print(\"No 'N' found in the 'RBD nucleotide' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0cac3c-47a6-4445-beca-b698694308cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: sars_cov_2_Alpha, Unique EPI_ID Count: 1043401\n",
      "Label: sars_cov_2_Beta, Unique EPI_ID Count: 14825\n",
      "Label: sars_cov_2_Delta, Unique EPI_ID Count: 3888578\n",
      "Label: sars_cov_2_Eris, Unique EPI_ID Count: 167378\n",
      "Label: sars_cov_2_Eta, Unique EPI_ID Count: 7079\n",
      "Label: sars_cov_2_FLiRT, Unique EPI_ID Count: 4277\n",
      "Label: sars_cov_2_Gamma, Unique EPI_ID Count: 113831\n",
      "Label: sars_cov_2_Kappa, Unique EPI_ID Count: 3594\n",
      "Label: sars_cov_2_Lambda, Unique EPI_ID Count: 9169\n",
      "Label: sars_cov_2_Mu, Unique EPI_ID Count: 11307\n",
      "Label: sars_cov_2_Omicron, Unique EPI_ID Count: 5450788\n",
      "Label: sars_cov_2_Theta, Unique EPI_ID Count: 85\n",
      "Label: sars_cov_2_Zeta, Unique EPI_ID Count: 6188\n"
     ]
    }
   ],
   "source": [
    "# Calculating the number of unique EPI IDs for each unique value of virus label_name\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_file_path = '/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/RBD_nucleotides_15mil_wo_nonvoc.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Group by label_name and count unique EPI_ID values\n",
    "label_counts = data.groupby('label_name')['EPI_ID'].nunique()\n",
    "\n",
    "# Print out the counts\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label: {label}, Unique EPI_ID Count: {count}\")\n",
    "    \n",
    "# Group by 'label_name' and count the number of sequences for each\n",
    "label_name_sequence_counts = data['label_name'].value_counts()\n",
    "\n",
    "# Print the unique label_name values and their corresponding sequence counts\n",
    "#for label_name, count in label_name_sequence_counts.items():\n",
    "    #print(f\"label_name: {label_name}, sequence count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d422e-3f7b-40be-9d19-43dedc9eb1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b818ca23-75bc-4b86-be03-191d0a880961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the output CSV: 500000\n",
      "Variant: sars_cov_2_Omicron, Number of rows: 100000\n",
      "Variant: sars_cov_2_Gamma, Number of rows: 100000\n",
      "Variant: sars_cov_2_Delta, Number of rows: 100000\n",
      "Variant: sars_cov_2_Alpha, Number of rows: 100000\n",
      "Variant: sars_cov_2_Eris, Number of rows: 100000\n"
     ]
    }
   ],
   "source": [
    "# Path1) Creating subset of above output csv with desired number of unique EPI_IDs for each hCOV19 variant label without complementary sequence\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def filter_and_write_csv(input_csv, output_csv, min_rows_per_variant=100000, max_rows_per_variant=100000):\n",
    "    # Initialize a dictionary to count the number of valid rows per variant\n",
    "    variant_counts = defaultdict(int)\n",
    "    variant_valid_rows = defaultdict(list)\n",
    "\n",
    "    with open(input_csv, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            sequence = row['sequence'].upper()\n",
    "\n",
    "            # Check if the sequence contains only A, T, G, C\n",
    "            if set(sequence) <= {'A', 'T', 'G', 'C'}:\n",
    "                variant = row['label_name']\n",
    "                variant_valid_rows[variant].append(row)\n",
    "\n",
    "    # Filter out variants that do not have at least min_rows_per_variant valid rows\n",
    "    valid_variants = {variant: rows for variant, rows in variant_valid_rows.items() if len(rows) >= min_rows_per_variant}\n",
    "\n",
    "    # Write to output CSV\n",
    "    with open(output_csv, 'w', newline='') as outfile:\n",
    "        fieldnames = ['EPI_ID', 'label_name', 'label_number', 'sequence']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        total_rows = 0\n",
    "        for variant, rows in valid_variants.items():\n",
    "            # Write only max_rows_per_variant rows for each variant\n",
    "            for row in rows[:max_rows_per_variant]:\n",
    "                writer.writerow(row)\n",
    "                variant_counts[variant] += 1\n",
    "                total_rows += 1\n",
    "\n",
    "    print(f\"Total number of rows in the output CSV: {total_rows}\")\n",
    "    for variant, count in variant_counts.items():\n",
    "        print(f\"Variant: {variant}, Number of rows: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/RBD_nucleotides_15mil_wo_nonvoc.csv\"\n",
    "    \n",
    "    main_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/\"\n",
    "\n",
    "    output_csv_path = os.path.join(main_dir, \"RBD_whole_nucleotides_15mil_wo_nonvoc_100k_epi.csv\")\n",
    "                                   \n",
    "    filter_and_write_csv(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae7de5b-a659-4852-83a1-b7dea9382db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the output CSV: 1000000\n",
      "Variant: sars_cov_2_Omicron, Number of rows: 200000\n",
      "Variant: sars_cov_2_Gamma, Number of rows: 200000\n",
      "Variant: sars_cov_2_Delta, Number of rows: 200000\n",
      "Variant: sars_cov_2_Alpha, Number of rows: 200000\n",
      "Variant: sars_cov_2_Eris, Number of rows: 200000\n"
     ]
    }
   ],
   "source": [
    "# Path2) Creating subset of above output csv with desired number of unique EPI_IDs for each hCOV19 variant label with complementary sequence\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from Bio.Seq import Seq  # Import Biopython's Seq class\n",
    "\n",
    "# Function to compute the reverse complement of a sequence using Biopython\n",
    "def reverse_complement(sequence):\n",
    "    return str(Seq(sequence).reverse_complement())  # Biopython's reverse complement\n",
    "\n",
    "def filter_and_write_csv(input_csv, output_csv, min_rows_per_variant=100000, max_rows_per_variant=100000):\n",
    "    # Initialize a dictionary to count the number of valid rows per variant\n",
    "    variant_counts = defaultdict(int)\n",
    "    variant_valid_rows = defaultdict(list)\n",
    "\n",
    "    with open(input_csv, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            sequence = row['sequence'].upper()\n",
    "\n",
    "            # Check if the sequence contains only A, T, G, C\n",
    "            if set(sequence) <= {'A', 'T', 'G', 'C'}:\n",
    "                variant = row['label_name']\n",
    "                variant_valid_rows[variant].append(row)\n",
    "\n",
    "    # Filter out variants that do not have at least min_rows_per_variant valid rows\n",
    "    valid_variants = {variant: rows for variant, rows in variant_valid_rows.items() if len(rows) >= min_rows_per_variant}\n",
    "\n",
    "    # Write to output CSV\n",
    "    with open(output_csv, 'w', newline='') as outfile:\n",
    "        fieldnames = ['EPI_ID', 'label_name', 'label_number', 'sequence']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        total_rows = 0\n",
    "        for variant, rows in valid_variants.items():\n",
    "            # Write only max_rows_per_variant rows for each variant\n",
    "            for row in rows[:max_rows_per_variant]:\n",
    "                original_sequence = row['sequence']\n",
    "                rev_complement_sequence = reverse_complement(original_sequence)\n",
    "\n",
    "                # Write original row\n",
    "                writer.writerow(row)\n",
    "\n",
    "                # Write reverse complement row\n",
    "                reverse_complement_row = row.copy()\n",
    "                reverse_complement_row['sequence'] = rev_complement_sequence\n",
    "                writer.writerow(reverse_complement_row)\n",
    "\n",
    "                variant_counts[variant] += 2  # Counting both original and reverse complement rows\n",
    "                total_rows += 2\n",
    "\n",
    "    print(f\"Total number of rows in the output CSV: {total_rows}\")\n",
    "    for variant, count in variant_counts.items():\n",
    "        print(f\"Variant: {variant}, Number of rows: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_path = \"/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/RBD_nucleotides_15mil_wo_nonvoc.csv\"\n",
    "    \n",
    "    main_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/\"\n",
    "\n",
    "    output_csv_path = os.path.join(main_dir, \"RBD_whole_nucleotides_15mil_wo_nonvoc_100k_epi_wt_comp.csv\")\n",
    "                                   \n",
    "    filter_and_write_csv(input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d93676-b2eb-4a9b-94fd-4e34d7f14ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Statistics:\n",
      "Total Sequences: 1000000\n",
      "Total Bases: 633000000\n",
      "Minimum Length: 633 bases\n",
      "Maximum Length: 633 bases\n",
      "Average Length: 633.00 bases\n"
     ]
    }
   ],
   "source": [
    "# Counting sequence statistics for trimmed RBD fasta file to find out if outliers exist\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def generate_sequence_statistics_from_csv(csv_file, sequence_column):\n",
    "    sequence_lengths = []\n",
    "    total_sequences = 0\n",
    "    total_bases = 0\n",
    "\n",
    "    # Load the CSV file and extract the sequences from the specified column\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Iterate over each sequence in the column\n",
    "    for sequence in df[sequence_column]:\n",
    "        total_sequences += 1\n",
    "        sequence_length = len(sequence)\n",
    "        sequence_lengths.append(sequence_length)\n",
    "        total_bases += sequence_length\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_length = min(sequence_lengths)\n",
    "    max_length = max(sequence_lengths)\n",
    "    average_length = total_bases / total_sequences\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"Sequence Statistics:\")\n",
    "    print(f\"Total Sequences: {total_sequences}\")\n",
    "    print(f\"Total Bases: {total_bases}\")\n",
    "    print(f\"Minimum Length: {min_length} bases\")\n",
    "    print(f\"Maximum Length: {max_length} bases\")\n",
    "    print(f\"Average Length: {average_length:.2f} bases\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/RBD_whole_nucleotides_15mil_wo_nonvoc_100k_epi_wt_comp.csv\"  # Replace with your CSV file path\n",
    "    sequence_column = \"sequence\"  # Replace with the column name that contains the sequences\n",
    "    generate_sequence_statistics_from_csv(csv_file, sequence_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543da99c-5f6f-4587-bb8d-c1b964c7c82f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Name: sars_cov_2_Alpha\n",
      "  - Unique Label Numbers: [3]\n",
      "  - Number of Unique EPI_IDs: 100000\n",
      "  - Number of Fragments (Sequences): 200000\n",
      "----------------------------------------\n",
      "Label Name: sars_cov_2_Delta\n",
      "  - Unique Label Numbers: [2]\n",
      "  - Number of Unique EPI_IDs: 100000\n",
      "  - Number of Fragments (Sequences): 200000\n",
      "----------------------------------------\n",
      "Label Name: sars_cov_2_Eris\n",
      "  - Unique Label Numbers: [4]\n",
      "  - Number of Unique EPI_IDs: 100000\n",
      "  - Number of Fragments (Sequences): 200000\n",
      "----------------------------------------\n",
      "Label Name: sars_cov_2_Gamma\n",
      "  - Unique Label Numbers: [1]\n",
      "  - Number of Unique EPI_IDs: 100000\n",
      "  - Number of Fragments (Sequences): 200000\n",
      "----------------------------------------\n",
      "Label Name: sars_cov_2_Omicron\n",
      "  - Unique Label Numbers: [0]\n",
      "  - Number of Unique EPI_IDs: 100000\n",
      "  - Number of Fragments (Sequences): 200000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generating counts for each variant label and total number of RBD sequences\n",
    "# Checking to see if label_names are correctly mapped to label_numbers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from CSV into a DataFrame\n",
    "rbd_df = pd.read_csv('/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/RBD_whole_nucleotides_15mil_wo_nonvoc_100k_epi_wt_comp.csv')\n",
    "\n",
    "# Function to check label numbers, count unique EPI_IDs, and count fragments for each label_name\n",
    "def check_label_numbers_and_count_epi_ids(df):\n",
    "    \"\"\"\n",
    "    Check the label_number values for each unique label_name value, print the number of EPI_ID values for each label_name,\n",
    "    and also print the number of fragments (sequences) for each label_name.\n",
    "    \"\"\"\n",
    "    # Group by label_name and analyze label_number, EPI_ID counts, and fragment counts\n",
    "    grouped = df.groupby('label_name')\n",
    "\n",
    "    for label_name, group in grouped:\n",
    "        unique_label_numbers = group['label_number'].unique()\n",
    "        epi_id_count = group['EPI_ID'].nunique()  # Count unique EPI_ID values\n",
    "        fragment_count = group.shape[0]  # Count total number of fragments (sequences)\n",
    "\n",
    "        print(f\"Label Name: {label_name}\")\n",
    "        print(f\"  - Unique Label Numbers: {unique_label_numbers}\")\n",
    "        print(f\"  - Number of Unique EPI_IDs: {epi_id_count}\")\n",
    "        print(f\"  - Number of Fragments (Sequences): {fragment_count}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Run the function on the DataFrame\n",
    "check_label_numbers_and_count_epi_ids(rbd_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196d602f-2a7d-4194-ae38-0583ed10551b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [01:51,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragments with reverse complements for all fragment lengths have been written to /mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/RBD_nucleotides_15mil_wo_nonvoc_20percent_overlaps_complementary_100k_epi.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#PATH3) Creating each one of 50, 75, 100, 150 and 250bps fragments with 50 bp overlaps from WGS generated above\n",
    "# Generating reverse complementary sequence for each generated fragment \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Function to generate overlapping fragments of varying lengths\n",
    "def generate_overlapping_fragments(sequence, fragment_len, overlap_ratio=0.2):\n",
    "    overlap = int(fragment_len * overlap_ratio)  # Calculate overlap as 20% of fragment length\n",
    "    fragments = []\n",
    "    for start in range(0, len(sequence) - fragment_len + 1, fragment_len - overlap):\n",
    "        fragment = sequence[start:start + fragment_len]\n",
    "        if len(fragment) == fragment_len:\n",
    "            fragments.append(fragment)\n",
    "    return fragments\n",
    "\n",
    "# Prompt user for the main directory path\n",
    "input_dir = \"/mmfs1/projects/changhui.yan/DeewanB/recent_hcov19_MSA/msaCodon_0522/\"\n",
    "output_dir = \"/mmfs1/projects/changhui.yan/DeewanB/DNABert2_rnaseq/genome_files/unfiltered_multiple_genomes/\"\n",
    "\n",
    "# Path to the input and output CSV files\n",
    "input_csv_path = os.path.join(input_dir, \"RBD_nucleotides_15mil_wo_nonvoc_100k_epi.csv\")\n",
    "output_csv_path = os.path.join(output_dir, \"RBD_nucleotides_15mil_wo_nonvoc_20percent_overlaps_complementary_100k_epi.csv\")\n",
    "\n",
    "# List of fragment lengths to generate\n",
    "fragment_lengths = [50, 150, 250]\n",
    "\n",
    "# Read the input CSV file in chunks\n",
    "chunksize = 1000  # Adjust the chunk size as needed\n",
    "input_columns = [\"EPI_ID\", \"label_name\", \"label_number\", \"sequence\"]\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_csv_path, mode='w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"EPI_ID\", \"label_name\", \"label_number\", \"fragment_length\", \"sequence\"])  # Write header\n",
    "    \n",
    "    # Process the input CSV file in chunks\n",
    "    for chunk in tqdm(pd.read_csv(input_csv_path, usecols=input_columns, chunksize=chunksize)):\n",
    "        for index, row in chunk.iterrows():\n",
    "            epi_id = row[\"EPI_ID\"]\n",
    "            variant_label = row[\"label_name\"]\n",
    "            variant_label_number = row[\"label_number\"]\n",
    "            sequence = row[\"sequence\"]\n",
    "            \n",
    "            # Generate and write fragments for each fragment length\n",
    "            for fragment_len in fragment_lengths:\n",
    "                fragments = generate_overlapping_fragments(sequence, fragment_len)\n",
    "                \n",
    "                for fragment in fragments:\n",
    "                    # Write the original fragment with the fragment length\n",
    "                    writer.writerow([epi_id, variant_label, variant_label_number, fragment_len, fragment])\n",
    "                    \n",
    "                    # Generate and write the reverse complementary fragment\n",
    "                    reverse_complement = str(Seq(fragment).reverse_complement())\n",
    "                    writer.writerow([epi_id, variant_label, variant_label_number, fragment_len, reverse_complement])\n",
    "\n",
    "print(f\"Fragments with reverse complements for all fragment lengths have been written to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6f772-c017-418d-ad07-907b5eae5e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dea94a-98c0-4205-9a65-33c25151791f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3.0",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
